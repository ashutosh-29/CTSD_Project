{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQl1NmFrvd0g",
        "outputId": "fa640f04-e284-4926-8b2b-36e8186895a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 370s 16s/step - loss: 0.7632 - accuracy: 0.5057 - val_loss: 0.7043 - val_accuracy: 0.5267\n",
            "Epoch 2/3\n",
            "22/22 [==============================] - 345s 16s/step - loss: 0.7269 - accuracy: 0.4543 - val_loss: 0.6922 - val_accuracy: 0.5267\n",
            "Epoch 3/3\n",
            "22/22 [==============================] - 351s 16s/step - loss: 0.7068 - accuracy: 0.4971 - val_loss: 0.6928 - val_accuracy: 0.5267\n",
            "10/10 [==============================] - 38s 4s/step - loss: 0.6928 - accuracy: 0.5267\n",
            "Test loss: 0.69\n",
            "Test accuracy: 0.53\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import TFBertModel, BertTokenizerFast\n",
        "import pandas as pd\n",
        "# Define hyperparameters\n",
        "max_length = 51\n",
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv('hindi.csv')\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(len(data) * 0.7)\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]\n",
        "\n",
        "# Tokenize text data using the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "train_encodings = tokenizer(train_data['text'].tolist(), max_length=max_length, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_data['text'].tolist(), max_length=max_length, padding=True, truncation=True)\n",
        "\n",
        "# Convert encodings to TensorFlow tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_data['label'].tolist()\n",
        ")).shuffle(len(train_data)).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_data['label'].tolist()\n",
        ")).batch(batch_size)\n",
        "\n",
        "# Load the BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Define the model architecture\n",
        "input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "embedding_layer = bert_model(input_ids, attention_mask)[0]\n",
        "x = GlobalAveragePooling1D()(embedding_layer)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test loss: {loss:.2f}')\n",
        "print(f'Test accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFUuAW59vkmJ",
        "outputId": "a11e2921-7889-4e55-f1b3-68fd56ab68c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of samples and the number of labels\n",
        "num_samples = 1000\n",
        "num_labels = 2\n",
        "\n",
        "# Create some random text data in Hindi\n",
        "text_data = [' '.join(np.random.choice(['नमस्ते', 'दुनिया', 'फू', 'बार'], size=np.random.randint(10, 20))) for i in range(num_samples)]\n",
        "\n",
        "# Create some random labels\n",
        "labels = np.random.randint(num_labels, size=num_samples)\n",
        "\n",
        "# Combine the text data and labels into a DataFrame\n",
        "data = pd.DataFrame({'text': text_data, 'label': labels})\n",
        "\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('hindi.csv', index=False)\n"
      ],
      "metadata": {
        "id": "NE2rGel8vuLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2f9P7jdnxgCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}